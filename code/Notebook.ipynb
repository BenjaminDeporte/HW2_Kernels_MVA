{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c1e49c",
   "metadata": {},
   "source": [
    "# Exercice 3 - Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb08c5f",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3853bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy import optimize\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plotClassification, plotRegression, plot_multiple_images, generateRings, scatter_label_points, loadMNIST\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b62e00",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The file 'classification_datasets' contains 3 small classification datasets:\n",
    "    \n",
    "    - dataset_1: mixture of two well separated gaussians\n",
    "    - dataset_2: mixture of two gaussians that are not separeted\n",
    "    - dataset_3: XOR dataset that is non-linearly separable.\n",
    "   \n",
    "Each dataset is a hierarchical dictionary with the following structure:\n",
    "        \n",
    "        dataset = {'train': {'x': data, 'y':label}\n",
    "                    'test': {'x': data, 'y':label}\n",
    "                  }\n",
    "The data $x$ is an $N$ by $2$ matrix, while the label $y$ is a vector of size $N$. \n",
    "\n",
    "Only the third dateset is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60200f67",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plotClassification() got an unexpected keyword argument 'ax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (name, dataset) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(datasets\u001b[38;5;241m.\u001b[39mitems()):    \n\u001b[0;32m----> 7\u001b[0m     \u001b[43mplotClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     ax[i]\u001b[38;5;241m.\u001b[39mset_title(name)\n",
      "\u001b[0;31mTypeError\u001b[0m: plotClassification() got an unexpected keyword argument 'ax'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = open('datasets/classification_datasets', 'rb')\n",
    "datasets = pkl.load(file)\n",
    "file.close()\n",
    "fig, ax = plt.subplots(1,3, figsize=(20, 5))\n",
    "\n",
    "for i, (name, dataset) in enumerate(datasets.items()):    \n",
    "    plotClassification(dataset['train']['x'], dataset['train']['y'], ax=ax[i])\n",
    "    ax[i].set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db8dd7",
   "metadata": {},
   "source": [
    "## III- Kernel SVC \n",
    "### 1- Implementing the Gaussian Kernel\n",
    "Implement the method 'kernel' of the class RBF and linear below, which takes as input two data matrices $X$ and $Y$ of size $N\\times d$ and $M\\times d$ and returns a gramm matrix $G$ of shape $N\\times M$ whose components are $k(x_i,y_j) = \\exp(-\\Vert x_i-y_j\\Vert^2/(2\\sigma^2))$ for the RBF kernel and $k(x_i,y_j)= x_i^{\\top}y_j$ for the linear kernel. (The fastest solution does not use any for loop!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f1edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, sigma=1.):\n",
    "        self.sigma = sigma  ## the variance of the kernel\n",
    "    def kernel(self,X,Y):\n",
    "        ## Input vectors X and Y of shape Nxd and Mxd\n",
    "        squared_distances = np.array([ [ np.sum((x[i,:]-y[j,:])**2) for j in range(y.shape[0]) ] for i in range(x.shape[0])])\n",
    "        return np.exp( -1/(2*sigma**2) * squared_distances )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91496c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def kernel(self,X,Y):\n",
    "        ## Input vectors X and Y of shape Nxd and Mxd\n",
    "        return  X @ Y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bbd4b",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e26bc",
   "metadata": {},
   "source": [
    "Representer theorem :\n",
    "\n",
    "$f(x) = \\sum_{i=1}^n \\alpha_i \\textbf{K}(x_i,x)$\n",
    "\n",
    "Lagrangien :\n",
    "\n",
    "$\\mathcal{L}(\\alpha,\\xi,b, \\mu,\\nu) = \\frac{1}{2}\\alpha^T \\textbf{K} \\alpha + C \\sum_{i=1}^{n}\\xi_i - \\sum_{i=1}^{n} \\mu_i \\left( y_i ( [\\textbf{K}\\alpha]_i + b) + \\xi_i - 1 \\right) - \\sum_{i=1}^{n} \\nu_i \\xi_i$\n",
    "\n",
    "Gradients :\n",
    "\n",
    "$\\nabla_\\alpha \\mathcal{L} = \\textbf{K} \\left( \\alpha - \\text{diag}(y)\\mu \\right)$\n",
    "\n",
    "$\\nabla_\\xi \\mathcal{L} = C \\textbf{1} - (\\nu + \\mu)$\n",
    "\n",
    "$\\nabla_b \\mathcal{L} = \\mu^T y$\n",
    "\n",
    "Optimal points :\n",
    "\n",
    "$\\alpha^* = \\text{diag}(y) \\mu$\n",
    "\n",
    "$\\mu + \\nu = C\\textbf{1}$\n",
    "\n",
    "$\\mu^T y = 0$\n",
    "\n",
    "Dual function :\n",
    "\n",
    "$g(\\nu,\\mu) = \\underset{\\alpha, \\xi, b}{\\text{inf}}\\,\\,\\mathcal{L} = -\\frac{1}{2} \\text{diag}(y) \\mu^T \\textbf{K} \\mu \\text{diag}(y) + \\mu^T \\textbf{1}$\n",
    "\n",
    "st $\\mu + \\nu = C$ et $\\mu^T y = 0$\n",
    "\n",
    "Complementary slackness :\n",
    "\n",
    "$\\alpha_i \\left( y_i (f(x_i) + b) + \\xi_i - 1 \\right) = 0, \\,\\, \\forall i$\n",
    "\n",
    "$\\left( C y_i - \\alpha_i\\right) \\xi_i = 0, \\,\\, \\forall i$\n",
    "\n",
    "Dual problem :\n",
    "\n",
    "$ \\underset{\\alpha}{\\text{min}} \\,\\, \\frac{1}{2}\\alpha^T \\textbf{K} \\alpha - \\alpha^T y $\n",
    "\n",
    "st : $0 \\leq \\alpha_i y_i \\leq C, \\,\\, \\forall i$ et $ \\alpha^T y = 0$\n",
    "\n",
    "(NB : on utilise le fait que $\\alpha_i = y_i \\mu_i \\implies \\mu_i = \\alpha_i y_i \\,\\, \\forall i$ puisque $y_i = \\pm 1$...)\n",
    "\n",
    "Support vector points :\n",
    "\n",
    "$\\alpha_i = 0 \\implies \\xi=0, \\,\\, y_i (f(x_i)+b) > 1$  point bien classé\n",
    "\n",
    "$\\alpha_i y_i = C \\implies \\xi_i >0, \\,\\,  y_i (f(x_i)+b) < 1$ point avec marge inférieure à 1\n",
    "\n",
    "$0 < \\alpha_i y_i < C \\implies y_i (f(x_i)+b) = 1$ support vector\n",
    "\n",
    "Parameter b :\n",
    "\n",
    "Sur l'ensemble $\\mathcal{S}$ des support vectors, on résoud $y_i (f(x_i)+b) = 1$ puis on moyenne, d'où $b = \\frac{1}{\\vert \\mathcal{S} \\vert} \\sum_{\\mathcal{S}} \\left( \\frac{1}{y_i} - \\textbf{K}\\alpha \\vert_i \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c925020",
   "metadata": {},
   "source": [
    "\n",
    "### 2- Implementing the classifier\n",
    "Implement the methods 'fit' and 'separating_function' of the class KernelSVC below to learn the Kernel Support Vector Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae012f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVC:\n",
    "    \n",
    "    def __init__(self, C, kernel, epsilon = 1e-3):\n",
    "        self.type = 'non-linear'\n",
    "        self.C = C                               \n",
    "        self.kernel = kernel        \n",
    "        self.alpha = None\n",
    "        self.support = None # support vectors\n",
    "        self.epsilon = epsilon\n",
    "        self.norm_f = None\n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #### You might define here any variable needed for the rest of the code\n",
    "        N = len(y)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # compute gram matrix, we might need it :-)\n",
    "        self.gram = self.kernel(X,X)\n",
    "        # vector of ones, size N\n",
    "        ones = np.ones(N)\n",
    "        # matrix NxN of y_i on diagonal\n",
    "        Dy = np.diag(y)\n",
    "\n",
    "        # Lagrange dual problem\n",
    "        def loss(alpha):\n",
    "            objective_function = 1/2 * alpha @ self.gram @ alpha - alpha @ y\n",
    "            return  objective_function\n",
    "\n",
    "        # Partial derivate of Ld on alpha\n",
    "        def grad_loss(alpha):\n",
    "            gradient = self.gram @ alpha - y\n",
    "            return gradient\n",
    "        \n",
    "        # Constraints on alpha of the shape :\n",
    "        # -  d - C*alpha  = 0\n",
    "        # -  b - A*alpha >= 0\n",
    "\n",
    "        fun_eq = lambda alpha: alpha @ y        \n",
    "        jac_eq = lambda alpha: y\n",
    "        fun_ineq_1 = lambda alpha: Dy @ alpha    \n",
    "        jac_ineq_1 = lambda alpha: Dy  # '''---------------jacobian wrt alpha of the  inequality constraint-------------------'''\n",
    "        fun_ineq_2 = lambda alpha : C * ones - Dy @ alpha\n",
    "        jac_ineq_2 = lambda alpha: -Dy  # '''---------------jacobian wrt alpha of the  inequality constraint-------------------'''\n",
    "        # fun_ineq = lambda alpha: np.concatenate((fun_ineq_1(alpha), fun_ineq_2(alpha)))\n",
    "        \n",
    "        constraints = ( [{'type': 'eq', 'fun': fun_eq, 'jac': jac_eq},\n",
    "                        {'type': 'ineq', 'fun': fun_ineq_1, 'jac': jac_ineq_1},\n",
    "                        {'type': 'ineq', 'fun': fun_ineq_2, 'jac': jac_ineq_2}]\n",
    "                        )\n",
    "\n",
    "        optRes = optimize.minimize(fun=lambda alpha: loss(alpha),\n",
    "                                   x0=np.ones(N), \n",
    "                                   method='SLSQP', \n",
    "                                   jac=lambda alpha: grad_loss(alpha), \n",
    "                                   constraints=constraints)\n",
    "        self.alpha = optRes.x\n",
    "\n",
    "        ## Assign the required attributes\n",
    "    \n",
    "        #'''------------------- A matrix with each row corresponding to support vectors ------------------'''\n",
    "        # list of indices of support vectors in dataset, None if not a support vector\n",
    "        self.indices_support = np.array([ i if (0 < self.alpha[i]*y[i]) and (self.alpha[i]*y[i] < C) else None for i in range(N) ])\n",
    "        # support vectors\n",
    "        self.support = np.array([ self.alpha[i] if self.indices_support[i] is not None else None for i in range(N) ]) \n",
    "        self.support = self.support[self.support != None]\n",
    "        # data points of support vectors\n",
    "        self.X_sv = np.zeros((self.support.shape[0],self.X.shape[1]))\n",
    "        isv = 0\n",
    "        for i in range(N):\n",
    "            if self.indices_support[i] is not None:\n",
    "                self.X_sv[isv,:] = X[i]\n",
    "                isv += 1\n",
    "        #'''------------------- b offset of the classifier ------------------'''\n",
    "        b = 0\n",
    "        Ka = self.gram @ self.alpha\n",
    "        for i in range(N):\n",
    "            if self.indices_support[i] is not None:\n",
    "                b += 1/y[i] - Ka[i]\n",
    "        self.b = b / len(self.support)\n",
    "        # '''------------------------RKHS norm of the function f ------------------------------'''\n",
    "        self.norm_f = 1/2 * self.alpha @ self.gram @ self.alpha\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    ### Implementation of the separting function $f$ \n",
    "    def separating_function(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        return self.kernel(x, self.X_sv) @ self.support + self.b\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        d = self.separating_function(X)\n",
    "        return 2 * (d+self.b> 0) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129641d-a03c-4d4c-85fd-52657901c5a1",
   "metadata": {},
   "source": [
    "# 2 b- Implementing the visualization function\n",
    "Implement the function plotClassification that takes new data as input and the model, then displays separating function and margins along with misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b807d9-df4c-4335-aaa6-a9bda19ddb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as pltcolors\n",
    "import seaborn as sns\n",
    "\n",
    "def plotHyperSurface(ax, xRange, model, intercept, label, color='grey', linestyle='-', alpha=1.):\n",
    "    #xx = np.linspace(-1, 1, 100)\n",
    "    if model.type=='linear':\n",
    "        xRange = np.array(xRange)\n",
    "        yy = -(model.w[0] / model.w[1]) * xRange  - intercept/model.w[1]\n",
    "        ax.plot(xRange, yy, color=color, label=label, linestyle=linestyle)\n",
    "    else:\n",
    "        xRange = np.linspace(xRange[0], xRange[1], 100)\n",
    "        X0, X1 = np.meshgrid(xRange, xRange)\n",
    "        xy = np.vstack([X0.ravel(), X1.ravel()]).T\n",
    "        Y30 = model.separating_function(xy).reshape(X0.shape) + intercept\n",
    "        ax.contour(X0, X1, Y30, colors=color, levels=[0.], alpha=alpha, linestyles=[linestyle]);\n",
    "\n",
    "\n",
    "def plotClassification(X, y, model=None, label='',  separatorLabel='Separator', \n",
    "            ax=None, bound=[[-1., 1.], [-1., 1.]]):\n",
    "    \"\"\" Plot the SVM separation, and margin \"\"\"\n",
    "    colors = ['blue','red']\n",
    "    labels = [1,-1]\n",
    "    cmap = pltcolors.ListedColormap(colors)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, figsize=(11, 7))\n",
    "    for k, label in enumerate(labels):\n",
    "        im = ax.scatter(X[y==label,0], X[y==label,1],  alpha=0.5,label='class '+str(label))\n",
    "\n",
    "    if model is not None:\n",
    "        # Plot the seprating function\n",
    "        plotHyperSurface(ax, bound[0], model, model.b, separatorLabel)\n",
    "        if model.support is not None:\n",
    "            ax.scatter(model.support[:,0], model.support[:,1], label='Support', s=80, facecolors='none', edgecolors='r', color='r')\n",
    "            print(\"Number of support vectors = %d\" % (len(model.support)))\n",
    "        \n",
    "        # Plot the margins\n",
    "        intercept_neg = ### compute the intercept for the negative margin\n",
    "        intercept_pos = ### compute the intercept for the positive margin\n",
    "        xx = np.array(bound[0])\n",
    "        plotHyperSurface(ax, xx, model, intercept_neg , 'Margin -', linestyle='-.', alpha=0.8)\n",
    "        plotHyperSurface(ax, xx, model, intercept_pos , 'Margin +', linestyle='--', alpha=0.8)\n",
    "            \n",
    "        # Plot points on the wrong side of the margin\n",
    "        wrong_side_points = # find wrong points\n",
    "        ax.scatter(wrong_side_points[:,0], wrong_side_points[:,1], label='Beyond the margin', s=80, facecolors='none', \n",
    "               edgecolors='grey', color='grey')  \n",
    "        \n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid()\n",
    "    ax.set_xlim(bound[0])\n",
    "    ax.set_ylim(bound[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86328951",
   "metadata": {},
   "source": [
    "### 3- Fitting the classifier\n",
    "\n",
    "Run the code block below to fit the classifier and report its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff8e38",
   "metadata": {},
   "source": [
    "### Dataset 1\n",
    "#### Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4963bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=1.\n",
    "kernel = Linear().kernel\n",
    "model = KernelSVC(C=C, kernel=kernel, epsilon=1e-14)\n",
    "train_dataset = datasets['dataset_1']['train']\n",
    "model.fit(train_dataset['x'], train_dataset['y'])\n",
    "plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbbe52",
   "metadata": {},
   "source": [
    "#### Gaussian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae87426",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.5\n",
    "C=1.\n",
    "kernel = RBF(sigma).kernel\n",
    "model = KernelSVC(C=C, kernel=kernel, epsilon=1e-14)\n",
    "train_dataset = datasets['dataset_1']['train']\n",
    "model.fit(train_dataset['x'], train_dataset['y'])\n",
    "plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45732d89",
   "metadata": {},
   "source": [
    "### Dataset 2\n",
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=.1\n",
    "kernel = Linear().kernel\n",
    "model = KernelSVC(C=C, kernel=kernel, epsilon=1e-14)\n",
    "train_dataset = datasets['dataset_2']['train']\n",
    "model.fit(train_dataset['x'], train_dataset['y'])\n",
    "plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fcda5",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a801b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.5\n",
    "C=1.\n",
    "kernel = RBF(sigma).kernel\n",
    "model = KernelSVC(C=C, kernel=kernel, epsilon=1e-14)\n",
    "train_dataset = datasets['dataset_2']['train']\n",
    "model.fit(train_dataset['x'], train_dataset['y'])\n",
    "plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e72a0",
   "metadata": {},
   "source": [
    "### Dataset 3\n",
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=1.\n",
    "kernel = Linear().kernel\n",
    "model = KernelSVC(C=C, kernel=kernel, epsilon=1e-14)\n",
    "train_dataset = datasets['dataset_3']['train']\n",
    "model.fit(train_dataset['x'], train_dataset['y'])\n",
    "plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546d52e",
   "metadata": {},
   "source": [
    "#### Gaussian SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bfbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.5\n",
    "C=100.\n",
    "kernel = RBF(sigma).kernel\n",
    "model = KernelSVC(C=C, kernel=kernel)\n",
    "train_dataset = datasets['dataset_3']['train']\n",
    "model.fit(train_dataset['x'], train_dataset['y'])\n",
    "plotClassification(train_dataset['x'], train_dataset['y'], model, label='Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
